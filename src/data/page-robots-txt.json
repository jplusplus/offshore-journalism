{
  "uid": "robots-txt",
  "category": "Case studies",
  "name": "Robots.txt",
  "file": "https://docs.google.com/document/d/1sGMFOLyhF5Hnqe7VOmhPhJc8UoD7EfqiKYzh2aEnJHw/pub",
  "excerpt": "Tatooine mothma skywalker calrissian fett. Amidala skywalker utapau antilles. Luuke antilles wedge mara",
  "html": "Hiding news items in robots.txt<p>Search engines, whether mainstream ones such as Google or specialized ones such as the Internet Archive, which, as the name implies, archives the web, operate by browsing vast quantities of web pages and indexing their contents. The browsing is done by automated scripts known as web crawlers. In order to avoid abuses, such as crawlers accessing the same page several times in a row, programmers of scripts and websites owners sat together and <a href=\"https://www.google.com/url?q=http://www.robotstxt.org/orig.html&amp;sa=D&amp;ust=1490294204918000&amp;usg=AFQjCNFpHKsLzw2BqkT6lMZovWqqML1yhg\">agreed on</a> a series of rules, to be written in a file called robots.txt. The year was 1994 and the world wide web was <a href=\"https://www.google.com/url?q=http://www.internetlivestats.com/total-number-of-websites/&amp;sa=D&amp;ust=1490294204919000&amp;usg=AFQjCNEDiN9mfDO9uNaEIK8jofEuAVx_BQ\">made of less</a> than 3,000 websites.</p><p>In the robots.txt file, website owners can give specific crawlers a series of orders. They can tell the Google crawlers to avoid a specific section of the site, they can prevent a specific crawler from indexing the whole site or they can tell all crawlers to avoid a specific page. Such files must not be used to prevent content from being indexed, as Google makes clear (they even <a href=\"https://www.google.com/url?q=https://support.google.com/webmasters/answer/6062608?hl%3Den&amp;sa=D&amp;ust=1490294204920000&amp;usg=AFQjCNHQHisk8xtzXtzAJkIqUvnEyWqr9g\">wrote in bold</a> that one “should not use robots.txt as a means to hide web pages from Google Search results”). It is merely a code of good conduct written in a time long gone. </p><p>Despite these shortcomings, many publishers use robots.txt files to prevent some pages from being indexed. As a result, the pages cannot be archived by digital libraries such as Archive.org (some archiving projects ignore robots.txt files, such as archive.is). </p><p>The robot.txt files are public. We looked at the files of all organizations listed under the “Newspaper” category of DBpedia, the structured-data equivalent of Wikipedia. Out of 5,200 websites, 3,600 files were extracted (some URLs in DBpedia were erroneous and some did not have robots.txt files). We could not run an analysis by country for lack of means, but did an analysis by top-level domains (the “.it” or “.de” after the name of the website), which is a close enough approximation.</p><p>European newspapers exhibit vastly higher than average numbers of hidden articles, with Italy topping the list with 21 articles, on average, hidden at every newspaper. The averages hide wide discrepancies. La Repubblica, for instance, hides over 200 articles, Corriere della Sera over 100.</p><p></p><p>The topics of the articles being prevented from being archived cover the usual “right to be forgotten” stories, where persons under suspicion or sentenced for a given crime are named. It is possible that the article in question pop up on top of search results on the name of the persons, making their job or partner searches much too difficult. However, many articles being removed from the archive deal with informations in the public interest. Some report accusations of child molestation against a former athlete (charges were later dropped because of the statute of limitations). Others have to do with a retirement home where patients were left in their own excrements for days. Others still report on companies filing for bankruptcy. Many articles were also depublished, which means, in the absence of an archived version, that the public will never know what they were about if they were not published in the paper version of the news outlet.</p>"
}